<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>线性回归 - 魔法沉思录</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="魔法沉思录"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="魔法沉思录"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="1. 线性回归模型给定数据集 $D &amp;#x3D; { (x_1,y_1),(x_2,y_2) …,(x_n,y_n)}$, 其中 $x_i &amp;#x3D; (x_i^1, x_i^2, … x_i^m)$。 线性回归是试图使用一个线性模型来尽可能的预测实际数值的输出标记。"><meta property="og:type" content="blog"><meta property="og:title" content="线性回归"><meta property="og:url" content="https://codeflysafe.github.io/2020/12/26/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"><meta property="og:site_name" content="魔法沉思录"><meta property="og:description" content="1. 线性回归模型给定数据集 $D &amp;#x3D; { (x_1,y_1),(x_2,y_2) …,(x_n,y_n)}$, 其中 $x_i &amp;#x3D; (x_i^1, x_i^2, … x_i^m)$。 线性回归是试图使用一个线性模型来尽可能的预测实际数值的输出标记。"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://images.pexels.com/photos/1714208/pexels-photo-1714208.jpeg?auto=compress&amp;cs=tinysrgb&amp;dpr=2&amp;w=500"><meta property="article:published_time" content="2020-12-26T08:14:19.000Z"><meta property="article:modified_time" content="2022-02-18T08:38:16.776Z"><meta property="article:author" content="sjhuang"><meta property="article:tag" content="machine learning"><meta property="article:tag" content="regression"><meta property="article:tag" content="linear regression"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://images.pexels.com/photos/1714208/pexels-photo-1714208.jpeg?auto=compress&amp;cs=tinysrgb&amp;dpr=2&amp;w=500"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://codeflysafe.github.io/2020/12/26/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"},"headline":"线性回归","image":[],"datePublished":"2020-12-26T08:14:19.000Z","dateModified":"2022-02-18T08:38:16.776Z","author":{"@type":"Person","name":"sjhuang"},"publisher":{"@type":"Organization","name":"魔法沉思录","logo":{"@type":"ImageObject","url":"https://codeflysafe.github.io/img/logo.svg"}},"description":"1. 线性回归模型给定数据集 $D &#x3D; { (x_1,y_1),(x_2,y_2) …,(x_n,y_n)}$, 其中 $x_i &#x3D; (x_i^1, x_i^2, … x_i^m)$。 线性回归是试图使用一个线性模型来尽可能的预测实际数值的输出标记。"}</script><link rel="canonical" href="https://codeflysafe.github.io/2020/12/26/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><!--!--><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><meta name="generator" content="Hexo 5.4.0"></head><body class="is-1-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="魔法沉思录" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a><a class="navbar-item" href="/love">Love</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-12"><div class="card"><div class="card-image"><span class="image is-7by3"><img class="fill" src="https://images.pexels.com/photos/1714208/pexels-photo-1714208.jpeg?auto=compress&amp;cs=tinysrgb&amp;dpr=2&amp;w=500" alt="线性回归"></span></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2020-12-26T08:14:19.000Z" title="2020/12/26 下午4:14:19">2020-12-26</time>发表</span><span class="level-item"><time dateTime="2022-02-18T08:38:16.776Z" title="2022/2/18 下午4:38:16">2022-02-18</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/machine-learning/">machine learning</a></span><span class="level-item">7 分钟读完 (大约1116个字)</span></div></div><h1 class="title is-3 is-size-4-mobile">线性回归</h1><div class="content"><p><img src="https://s3.bmp.ovh/imgs/2022/02/fc23066cd97819ea.png"><a name="b2xBz"></a></p>
<h2 id="1-线性回归模型"><a href="#1-线性回归模型" class="headerlink" title="1. 线性回归模型"></a>1. 线性回归模型</h2><p>给定数据集 $D = { (x_1,y_1),(x_2,y_2) …,(x_n,y_n)}$, 其中 $x_i = (x_i^1, x_i^2, … x_i^m)$。 线性回归是试图使用一个线性模型来尽可能的预测实际数值的输出标记。</p>
<span id="more"></span>

<p>假设，使用函数: </p>

$$
\begin{aligned}
y = w^Tx + b \\
w = \begin{bmatrix} w^1 \\ ...\\ w^m \end{bmatrix}
\end{aligned}
$$ 


<p>b 是一个 <code>scalar</code> 。<br><br />使用均方误差作为回归模型的性能度量，则可以转化为一个无约束的优化问题：</p>
<p> $\min_{w,b} L(w,b) = \frac{1}{2n}\sum_i^n(y_i - \hat{y}_i)^2$</p>
<p>解决这个优化问题，可以求导并令其偏导数为0，即可。下面是分别对 <code>w</code> , <code>b</code> 求导的结果</p>
<p>$$<br>\begin{aligned}<br> \frac{dL}{dw} &amp;= \frac{1}{2n}\sum_i^n 2(y_i-\hat{y}_i)\frac{dy_i}{dw}   \<br>&amp;= \frac{1}{n}\sum_i^n(y_i-\hat{y}_i)x_i \qquad (1)\<br>\frac{dL}{db} &amp;= \frac{1}{n}\sum_i^n(y_i - \hat{y}_i) \qquad (2)<br>\end{aligned}<br>$$</p>
<br />
<br />通常会把(1),(2) 式写成矩阵的形式，即:<br />
<br />

<p>$$<br>\begin{aligned}<br>\frac{dL}{dw}&amp;= \frac{1}{n}X^T(Xw - \hat{Y}) \qquad (3) \<br>\frac{dL}{db}&amp;= \frac{1}{n}I(Xw -\hat{Y})   \qquad (4) \<br>\end{aligned}<br>$$</p>
<p><br />其中 $X=\begin{bmatrix}x_1 \ x_2 \ … \ x_n \end{bmatrix}$, $\hat{Y}=\begin{bmatrix}\hat{y}_1 \ \hat{y}_2 \ … \ \hat{y}_n\end{bmatrix}$, $I=\begin{bmatrix}1,1 … 1\end{bmatrix}$<br /></p>
<p><br />令式（3）等于0， 可以得到 $w^{\star} = (X^TX)^{-1}X^T\hat{Y}$, 此时 $A = X^TX$ 必须是非奇异矩阵。<br />但是实际情况下，可能存在A并不是满秩的，比如特征的维度大于样本数。 因此，此时可能会接触多个w，能够使均方误差最小化。选择哪一个作为最优的w，由算法的归纳偏好来决定，比如引入正则化项。</p>
<p><a name="83Wqd"></a></p>
<h2 id="2-对数似然估计"><a href="#2-对数似然估计" class="headerlink" title="2. 对数似然估计"></a>2. 对数似然估计</h2><p><br />使用线性回归做分类问题，以二分类问题为例。 可以将其结果y映射到[0,1]区间，然后规定其值大于0.5作为正例，否则作为反例。 <br />如何选择映射函数 <code>f</code> 呢，这里可以采用 <code>sigmoid</code> 函数：<br /><br><br /><br>$$<br>\begin{aligned}<br> f &amp;= \sigma(z) = \frac{1}{1 + \exp{-z}} \<br> z &amp;= w^Tx + b \<br>\end{aligned}<br>$$<br><br />其曲线为：<br /><br><img src="https://s3.bmp.ovh/imgs/2022/02/629ee62f32647f97.png"><br><br />sigmoid 曲线<br />此时，将f看作是概率分布，可以采用最大似然概率作为性能度量（它等价于最小化交叉熵）<br />等价于优化问题：<br><br /></p>
<p>$$<br>\min_{w,b} L(w,b) = -\sum_i^n(\hat{y}_i\log(f(x_i))  + (1-\hat{y}_i)\log(1 - f(x_i)))<br>$$</p>
<br />
<br />分别对 w,b求偏导数可得：<br />


$$
\begin{aligned}
\frac{dL}{dw} &= -\sum_i^n(\frac{\hat{y}_i}{f}\frac{df}{dw} - \frac{1-\hat{y}_i}{1-f}\frac{df}{dw}) \\
&= -\sum_i^n(\frac{\hat{y}_i}{f}f(1-f)x_i - \frac{1-\hat{y}_i}{1-f}f(1-f)x_i )) \\
&= -\sum_i^n(\hat{y}_i - f)x_i \qquad (5) \\
\frac{dL}{db} &= -\sum_i^n(\hat{y}_i - f) \qquad (6)
\end{aligned}
$$


<p><br />分别对为w,b求偏导数<br />其中$f = f(x_i)$。 可以发现与线性回归的偏导数有一样的格式。</p>
<p><a name="1xotj"></a></p>
<h2 id="3-LDA-线性判别分析"><a href="#3-LDA-线性判别分析" class="headerlink" title="3. LDA 线性判别分析"></a>3. LDA 线性判别分析</h2><p><br />LDA（Linear Discriminant Analysis), 又称为 <code>fisher 判别分析</code> 。它的主要思想是，将给定的样本，投影到一条直线上，并且保证类内的投影点越接近越好，但是类间的投影点越分散越好。<br /><br><br />类内投影点，越接近越好，可以采用方差（协方差举证）指标来衡量，即使其同类样本之间的协反差尽可能的小。<br />对于类间的投影点，越分散越好。这里可以采用不同类别之间的均值相差尽可能的大。<br /><br><br />总结起来的公式为:<br /></p>
<p>$$\max_{w} J(w) = \frac{||w^T\mu_0 - w^T\mu_1||}{w^T\Sigma_0w + w^T\Sigma_1w}<br>$$</p>
<p><br />LDA的度量函数<br /><br><br />w 代表被投影的直线。</p>
<p>令$S_w = \Sigma_0 + \Sigma_1$,$S_b = (\mu_0 - \mu_1)(\mu_0 - \mu_1)^T$<br />则$J(w) = \frac{w^TS_bw}{w^TS_ww}$。<br />由于w代表被投影的直线，只与其方向有关，不妨设置$w^TS_ww = 1$, 则转化为以下有约束的优化问题</p>
<p>$$<br>\begin{aligned}<br>&amp;\min_{w}\quad -w^TS_bw \<br>&amp;s.t \quad w^TS_ww = 1<br>\end{aligned}$$</p>
<p><br />可以使用拉格朗日乘法公式，可得:<br /><br><br /></p>
<p>$$\begin{aligned}<br>L(w) &amp;= -w^TS_bw + \lambda(w^TS_ww - 1)  \<br>\frac{dL}{dw} &amp;= -2S_bw + 2\lambda S_ww = 0 \<br>&amp; S_w^{-1}S_bw = \lambda w<br>\end{aligned}<br>$$</p>
<br />

<p>易知，$\lambda$ 为矩阵 $S_w^{-1}S_w$的特征值和特征向量。</p>
<p>求解$S_w^{-1}$时，要考虑其稳定性，通常采用其SVD分解。即$S_w^{-1} = V\Sigma^{-1}U^T$。</p>
</div><div class="article-licensing box"><div class="licensing-title"><p>线性回归</p><p><a href="https://codeflysafe.github.io/2020/12/26/线性模型/">https://codeflysafe.github.io/2020/12/26/线性模型/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>sjhuang</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2020-12-26</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2022-02-18</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/machine-learning/">machine learning</a><a class="link-muted mr-2" rel="tag" href="/tags/regression/">regression</a><a class="link-muted mr-2" rel="tag" href="/tags/linear-regression/">linear regression</a></div><!--!--></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">喜欢这篇文章？打赏一下作者吧</h3><div class="buttons is-centered"><a class="button donate" href="/" target="_blank" rel="noopener" data-type="afdian"><span class="icon is-small"><i class="fas fa-charging-station"></i></span><span>爱发电</span></a><a class="button donate" data-type="alipay"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>支付宝</span><span class="qrcode"><img src="/" alt="支付宝"></span></a><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>微信</span><span class="qrcode"><img src="/" alt="微信"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2021/03/28/%E6%84%9F%E7%9F%A5%E6%9C%BA/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">感知机</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2020/10/10/SVM(2)_%20Soft-SVM/"><span class="level-item">Soft-SVM</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">评论</h3><div id="comment-container"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.css"><script src="https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.min.js"></script><script>var gitalk = new Gitalk({
            id: "31a3d71709b987baaa71bba4591e7fc3",
            repo: "gitalk",
            owner: "codeflysafe",
            clientID: "48d1eb3658ef4923896f",
            clientSecret: "d619d46a6c36396cab7dc0190ad1630bd3982b34",
            admin: ["codeflysafe"],
            createIssueManually: false,
            distractionFreeMode: false,
            perPage: 10,
            pagerDirection: "last",
            
            
            enableHotKey: true,
            
        })
        gitalk.render('comment-container')</script></div></div></div><!--!--><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="魔法沉思录" height="28"></a><p class="is-size-7"><span>&copy; 2022 sjhuang</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>