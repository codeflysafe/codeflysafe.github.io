<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title> Theory behind GAN - 魔法沉思录</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="魔法沉思录"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="魔法沉思录"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="GenerationTarget: find data distribution $$P_{data}(x)$$.x 是一个图片 ( a high-dimensional vector) Before GANTarget：使用一个分布 $P_G(x,\theta)$ 去拟合分布$P_{data}(x)$（固定的），使两者越接近越好. 例如$P_G(x,\theta)$可以是 gaussian mi"><meta property="og:type" content="blog"><meta property="og:title" content=" Theory behind GAN"><meta property="og:url" content="https://codeflysafe.github.io/2020/06/15/2020-06-15-Theory-behind-GAN/"><meta property="og:site_name" content="魔法沉思录"><meta property="og:description" content="GenerationTarget: find data distribution $$P_{data}(x)$$.x 是一个图片 ( a high-dimensional vector) Before GANTarget：使用一个分布 $P_G(x,\theta)$ 去拟合分布$P_{data}(x)$（固定的），使两者越接近越好. 例如$P_G(x,\theta)$可以是 gaussian mi"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://raw.githubusercontent.com/hsjfans/git_resource/master/img/20200615092048.png"><meta property="og:image" content="https://raw.githubusercontent.com/hsjfans/git_resource/master/img/20200615094212.png"><meta property="og:image" content="https://raw.githubusercontent.com/hsjfans/git_resource/master/img/20200615120036.png"><meta property="og:image" content="https://raw.githubusercontent.com/hsjfans/git_resource/master/img/20200615124406.png"><meta property="article:published_time" content="2020-06-15T04:56:39.000Z"><meta property="article:modified_time" content="2022-02-18T11:37:07.618Z"><meta property="article:author" content="sjhuang"><meta property="article:tag" content="machine-learning"><meta property="article:tag" content="gan"><meta property="article:tag" content="李宏毅"><meta property="article:tag" content="deep-learning"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://raw.githubusercontent.com/hsjfans/git_resource/master/img/20200615092048.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://codeflysafe.github.io/2020/06/15/2020-06-15-Theory-behind-GAN/"},"headline":" Theory behind GAN","image":["https://raw.githubusercontent.com/hsjfans/git_resource/master/img/20200615092048.png","https://raw.githubusercontent.com/hsjfans/git_resource/master/img/20200615094212.png","https://raw.githubusercontent.com/hsjfans/git_resource/master/img/20200615120036.png","https://raw.githubusercontent.com/hsjfans/git_resource/master/img/20200615124406.png"],"datePublished":"2020-06-15T04:56:39.000Z","dateModified":"2022-02-18T11:37:07.618Z","author":{"@type":"Person","name":"sjhuang"},"publisher":{"@type":"Organization","name":"魔法沉思录","logo":{"@type":"ImageObject","url":"https://codeflysafe.github.io/img/logo.svg"}},"description":"GenerationTarget: find data distribution $$P_{data}(x)$$.x 是一个图片 ( a high-dimensional vector) Before GANTarget：使用一个分布 $P_G(x,\\theta)$ 去拟合分布$P_{data}(x)$（固定的），使两者越接近越好. 例如$P_G(x,\\theta)$可以是 gaussian mi"}</script><link rel="canonical" href="https://codeflysafe.github.io/2020/06/15/2020-06-15-Theory-behind-GAN/"><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><!--!--><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><meta name="generator" content="Hexo 5.4.0"></head><body class="is-1-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="魔法沉思录" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a><a class="navbar-item" href="/love">Love</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-12"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2020-06-15T04:56:39.000Z" title="2020/6/15 下午12:56:39">2020-06-15</time>发表</span><span class="level-item"><time dateTime="2022-02-18T11:37:07.618Z" title="2022/2/18 下午7:37:07">2022-02-18</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/Generative-Adversial-Network/">Generative Adversial Network</a></span><span class="level-item">10 分钟读完 (大约1549个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"> Theory behind GAN</h1><div class="content"><h2 id="Generation"><a href="#Generation" class="headerlink" title="Generation"></a>Generation</h2><p>Target: find data distribution $$P_{data}(x)$$.<br><em>x 是一个图片 ( a high-dimensional vector)</em></p>
<h3 id="Before-GAN"><a href="#Before-GAN" class="headerlink" title="Before GAN"></a>Before GAN</h3><p><strong>Target</strong>：使用一个分布 $P_G(x,\theta)$ 去拟合分布$P_{data}(x)$（固定的），使两者越接近越好. 例如$P_G(x,\theta)$可以是 <code>gaussian mixture model</code>,此时 $\theta$为 <code>means</code> and <code>variances</code> of this model</p>
<ul>
<li>Given a data distribution $P_{data}(x)$ (We can sample from it)<blockquote>
<p>$P_{data}(x)$ 未知，但是可以 sample from it. (就是从已有的<code>database</code>中<code>sample</code>出来一些)</p>
</blockquote>
</li>
<li>We have a data distribution $P_G(x;\theta)$ parameterized by $\theta$<blockquote>
<p>We want to find $\theta$ such that $P_G$ close to $P_{data}$</p>
</blockquote>
</li>
<li>Sample {$x^1,x^2…,x^m$} from $P_{data}(x)$ and compute $P_G(x^i;\theta)$</li>
<li>Find $\theta^*$ maximizing the Object Function</li>
</ul>
<p><strong>Object Function</strong>：<code>Maximum Likelihood Estimation</code> (最大似然估计)</p>
<p>$$<br>\begin{aligned}<br>F(\theta) &amp;= \prod_{x=1}^{m} P_G(x^i;\theta)\tag{1}<br>\end{aligned}<br>$$</p>
<p>要使 $F(\theta)$ 达到最大，即求 $\theta^*$ 使$F(\theta)$取得最值</p>
<p>$$\begin{aligned}<br> \theta^* &amp;= \arg\mathop{\max}\limits_{\theta}F(\theta) \tag{2} \<br>          &amp;= \arg\mathop{\max}\limits_{\theta}\sum_{x=1}^mlog(P_G(x^i;\theta)) \<br>          &amp;\approx \color{red}\arg\mathop{\max}\limits_{\theta}E_x\sim_{P_{data}(x)}(log(P_G(x;\theta)))\<br>          &amp;= \color{red}\arg\mathop{\max}\limits_{\theta} (\int_x P_{data}(x)log(P_G(x;\theta))dx - \int_x P_{data}(x)log(P_{data}(x))dx) \<br>          &amp;= \arg\mathop{\max}\limits_{\theta}\int_xP_{data}(x)log(P_G(x;\theta)/P_{data}(x))dx \<br>          &amp;= \arg\mathop{\min}\limits_{\theta} KL Div(P_{data}||P_G)</p>
<p>\end{aligned}$$</p>
<p>即： Maximum Likelihood Estimation $\approx$ KL Divergence</p>
<p>因此 对于 <code>Generator</code> 的目的就是 $G^* = \arg\mathop{\min}\limits_{G}Div(P_G,P_{data})$,即使 $P_G 和 P_{data}$ 之间的散度最小。</p>
<h4 id="这里存在一个问题-How-to-define-general-P-G-x"><a href="#这里存在一个问题-How-to-define-general-P-G-x" class="headerlink" title="这里存在一个问题: How to define general $P_G(x)$ ?"></a>这里存在一个问题: How to define general $P_G(x)$ ?</h4><p>  对于 $P_G(x)$分布(更加 general distribution),它可能无法进行计算(如 <code>Nerual Network</code>),即 $logP_G(x;\theta)$无法计算.</p>
<h2 id="Using-GAN"><a href="#Using-GAN" class="headerlink" title="Using GAN"></a>Using GAN</h2><h3 id="GAN-是如何处理这个问题的呢？-Generator"><a href="#GAN-是如何处理这个问题的呢？-Generator" class="headerlink" title="GAN 是如何处理这个问题的呢？ (Generator)"></a>GAN 是如何处理这个问题的呢？ (Generator)</h3><p>GAN 采用了一个 <code>NN</code> 来拟合 $P_G(x)$，即 Generator 是一个 network, 使用它来定义分布 $P_G$</p>
<blockquote>
<p>To learn the generator’s distribution $P_G$,we define a prior on input noise variables $P_z(z)$, then represent a mapping to data space as $G(z;\theta_g)$,where G is a differentiable function represented by a multilayer perceptron with parameters $\theta_g$.</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/hsjfans/git_resource/master/img/20200615092048.png"></p>
<blockquote>
<p>prior distribution $P_z(z) 一般可以选取任意分布，如 Gussian Mixture Distribution$</p>
</blockquote>
<p>根据上面可知，它的 Object Function (loss) 是某种 <code>Divergence</code>, 即 $Div(P_G,P_{data})$,<br>而且目的为是这种散度最小，即 $G^* = \arg\mathop{\min}\limits_G Div({P_G,P_{data}})$.<br>关键就是<strong>如何计算这种 Divergence？</strong></p>
<h3 id="How-to-Compute-Divergence-Discriminator"><a href="#How-to-Compute-Divergence-Discriminator" class="headerlink" title="How to Compute Divergence (Discriminator) ?"></a>How to Compute Divergence (Discriminator) ?</h3><p>对于 $P_G$或者$P_{data}$ 因为是未知的，我们无法计算，但是可以 Sample from them.</p>
<blockquote>
<p>We alse define a second multilayer perceptron $D(x;\theta_d)$ that outputs a single scalar. D(x) represents the probability that x came from the data rather that $P_g$. We train D to maximize the probability of assigning the correct label to both training examples and samples from G.</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/hsjfans/git_resource/master/img/20200615094212.png"></p>
<h4 id="Object-Function-For-D"><a href="#Object-Function-For-D" class="headerlink" title="Object Function For D"></a>Object Function For D</h4><p>When train <code>D</code>, <code>G</code>is fixed:<br>   $$\begin{aligned}<br>     V(G,D) &amp;= E_x \sim_{P_{data}(x)}[log(D(x))] + E_z \sim_{P_z(z)}[log(1-D(G(z)))]  \<br>       &amp; = E_x \sim_{P_{data}(x)}[log(D(x))] + E_x \sim_{P_G(x)}[log(1-D(x))]  \tag{3}<br>   \end{aligned}<br>   $$</p>
<p>即 </p>
<p>   $$D^* = \arg\mathop{\max}\limits_DV(D,G) \tag{4}$$</p>
<h4 id="式-3-如何解释呢？"><a href="#式-3-如何解释呢？" class="headerlink" title="式(3)如何解释呢？"></a>式(3)如何解释呢？</h4><ol>
<li>当 x 从 $P_data(x)$ sample 出来的时，那就使 <code>scalar</code>(D(x)) 越大越好（因为它是真实的）</li>
<li>当 x 从 $P_G(x)$ sample 出来时，<code>saclar</code>(D(x))越小越好，即<code>1-D(x)</code> 越大越好（因为它是Generator出来的的）</li>
</ol>
<p>上面我们讲到，generator的目的使 $P_G$ 和 $P_{data}$ 的某种 Divergence 越小越好,其实对于式(3)它就等同于某种 Divergence</p>
<h4 id="证明-V-G-D-等同于-JS-Divergence"><a href="#证明-V-G-D-等同于-JS-Divergence" class="headerlink" title="证明 V(G,D) 等同于 JS-Divergence"></a>证明 V(G,D) 等同于 JS-Divergence</h4><ol>
<li>fixed G, 求解 $D^*$<br>$$<br> \begin{aligned}<br> D^* &amp;= \arg\mathop{\max}\limits_DV(D,G) \<pre><code> &amp;= \arg\mathop&#123;\max&#125;\limits_D[\int_xP_&#123;data&#125;(x)log(D(x))dx + \int_xP_G(x)log(1-D(x))dx ]\\
 &amp;= \arg\mathop&#123;\max&#125;\limits_D\int_x[P_&#123;data&#125;(x)log(D(x)) + P_G(x)log(1-D(x))]dx \tag&#123;5&#125;
</code></pre>
 \end{aligned}<br>$$</li>
</ol>
<ol start="2">
<li><p>Given x, $D^*$使 $P_{data}(x)log(D(x)) + P_G(x)log(1-D(x))$ maximum<br>简化为 $L(D) = alogD + blog(1-D)$ </p>
<p>Assumed: $a = P_{data}(x)$ and  $b = P_G(x)$ </p>
<p>$$\begin{aligned}<br>   &amp;L(D) = alogD + blog(1-D) \<br>   &amp;\frac{dL(D)}{dD} = a/D + b/(1-D)<em>(-1) = 0\<br>   &amp; D^</em> = a/(a+b) \tag{6}</p>
<p>\end{aligned}$$</p>
<blockquote>
<p>For any (a,b) $\in R^2$, function $y\to alog(y) + blog(1-y)$ achieves its maximum in [0,1] at $\frac{a}{a+b}$</p>
</blockquote>
</li>
<li><p>将 $D^* = a/(a+b)$ 带入到式（3）中，有:</p>
<p>$$\begin{aligned}<br>  V(G,D) &amp;= E_x \sim_{P_{data}(x)}[log(\frac{P_{data}(x)}{(P_{data}(x)+P_G(x))})] + E_x \sim_{P_G(x)}[log(\frac{P_G(x)}{(P_{data}(x)+P_G(x))})] \<br>  &amp;= \int_x[P_{data}(x)log(\frac{P_{data}(x)}{(P_{data}(x)+P_G(x))}) + P_G(x)log(\frac{P_G(x)}{(P_{data}(x)+P_G(x))})]dx \<br>  &amp;= \int_x[P_{data}(x)log(\frac{P_{data}(x)/2}{(P_{data}(x)+P_G(x))/2}) + P_G(x)log(\frac{P_G(x)/2}{(P_{data}(x)+P_G(x))/2})]dx \<br>  &amp;= -\int_x[P_{data}(x)log2 + P_G(x)log2]dx + \int_x[P_{data}(x)log(\frac{P_{data}(x)}{(P_{data}(x)+P_G(x))/2}) + P_G(x)log(\frac{P_G(x)}{(P_{data}(x)+P_G(x))/2})]dx \<br>  &amp;= -log4 + KLDiv(P_{data}(x)||(P_{data}(x)+P_G(x))/2) + KLDiv(P_G(x)||(P_{data}(x)+P_G(x))/2) \<br>  &amp;= -log4 + 2*\color{red}JSDiv(P_{data}||P_G)<br>\end{aligned}$$</p>
</li>
</ol>
<h4 id="计算-G-mathop-arg-min-limits-GDiv-P-data-P-G"><a href="#计算-G-mathop-arg-min-limits-GDiv-P-data-P-G" class="headerlink" title="计算 $G^* = \mathop{\arg\min}\limits_GDiv(P_{data},P_G)$"></a>计算 $G^* = \mathop{\arg\min}\limits_GDiv(P_{data},P_G)$</h4><p>上式就等同于:<br>  $$\begin{aligned}<br>      G^* &amp;= \arg\mathop{\min}\limits_GDiv(P_{data},P_G) \<br>          &amp;= \arg\mathop{\min}\limits_G\mathop{\max}\limits_DV(G,D) \<br>          &amp;= \arg\mathop{\min}\limits_GC(G)<br>  \end{aligned}$$</p>
<p>  其中 $C(G)= \arg\mathop{\max}\limits_DV(G,D)$</p>
<p>  <img src="https://raw.githubusercontent.com/hsjfans/git_resource/master/img/20200615120036.png"></p>
<h2 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h2><ol>
<li>Initialize generator and discriminator</li>
<li>In each training iteration:     <ol>
<li>Fix G, update D</li>
<li>Fix D, update G</li>
</ol>
</li>
</ol>
<h3 id="Procerss"><a href="#Procerss" class="headerlink" title="Procerss"></a>Procerss</h3><p>实际上：<br> $$\begin{aligned}<br>      V(G,D) &amp;= E_x \sim_{P_{data}(x)}[log(D(x))] + E_x \sim_{P_G(x)}[log(1-D(x))] \<br>             &amp;\approx \frac{1}{m}\sum_{i=1}^{m}logD(x^i) + \frac{1}{m}\sum_{i=1}^{m}log(1-D(\hat{x}^i))<br>  \end{aligned}$$ </p>
<p>initialize $\theta_d$ for D and $\theta_g$ for G</p>
<ul>
<li><p>In each training iteration</p>
</li>
<li><p>Training D, repeat K times</p>
<ul>
<li>sample m examples{$x^1,x^2,…x^m$} from database $P_{data}(x)$ </li>
<li>sample m examples{$z^1,z^2,…z^m$} from prior distribution $P_z(z)$</li>
<li>get generated data {$\hat{x}^1,\hat{x}^2,…\hat{x}^m$} via $G(z^i)$</li>
<li>fixed $\theta_g$,update $\theta_d$ to maximize:<br>$$\begin{aligned}<br>  \hat{V} &amp;= \frac{1}{m}\sum_{i=1}^{m}logD(x^i) + \frac{1}{m}\sum_{i=1}^{m}log(1-D(\hat{x}^i)) \<br>  \theta_d &amp;\gets \theta_d + \eta\bigtriangledown\hat{V}(\theta_d)<br>\end{aligned}$$ </li>
</ul>
</li>
<li><p>Training G, only once</p>
<ul>
<li>sample m examples{$z^1,z^2,…z^m$} from prior distribution $P_z(z)$</li>
<li>fixed $\theta_d$,update $\theta_g$ to minimize:<br>$$\begin{aligned}<br> \hat{V} &amp;= \frac{1}{m}\sum_{i=1}^{m}logD(x^i) + \frac{1}{m}\sum_{i=1}^{m}log(1-D(G(z^i))) \<br> &amp;\approx\frac{1}{m}\sum_{i=1}^{m}log(1-D(G(z^i))) \<br>  \theta_g &amp;\gets \theta_g - \eta\bigtriangledown\hat{V}(\theta_g)<br>\end{aligned}$$</li>
</ul>
</li>
</ul>
<p>实际上，在 Training G 时，采用的是：</p>
<p>$$\begin{aligned}<br>     \hat{V} &amp;=  -\frac{1}{m}\sum_{i=1}^{m}log(D(G(z^i))) \<br>      \theta_g &amp;\gets \theta_g - \eta\bigtriangledown\hat{V}(\theta_g)<br>\end{aligned}$$</p>
<p><img src="https://raw.githubusercontent.com/hsjfans/git_resource/master/img/20200615124406.png"></p>
</div><div class="article-licensing box"><div class="licensing-title"><p> Theory behind GAN</p><p><a href="https://codeflysafe.github.io/2020/06/15/2020-06-15-Theory-behind-GAN/">https://codeflysafe.github.io/2020/06/15/2020-06-15-Theory-behind-GAN/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>sjhuang</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2020-06-15</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2022-02-18</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/machine-learning/">machine-learning</a><a class="link-muted mr-2" rel="tag" href="/tags/gan/">gan</a><a class="link-muted mr-2" rel="tag" href="/tags/%E6%9D%8E%E5%AE%8F%E6%AF%85/">李宏毅</a><a class="link-muted mr-2" rel="tag" href="/tags/deep-learning/">deep-learning</a></div><!--!--></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">喜欢这篇文章？打赏一下作者吧</h3><div class="buttons is-centered"><a class="button donate" href="/" target="_blank" rel="noopener" data-type="afdian"><span class="icon is-small"><i class="fas fa-charging-station"></i></span><span>爱发电</span></a><a class="button donate" data-type="alipay"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>支付宝</span><span class="qrcode"><img src="/" alt="支付宝"></span></a><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>微信</span><span class="qrcode"><img src="/" alt="微信"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2020/09/22/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%92%8C%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">朴素贝叶斯</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2020/05/18/2020-05-18-Next-Step/"><span class="level-item">Next Step</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">评论</h3><div id="comment-container"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.css"><script src="https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.min.js"></script><script>var gitalk = new Gitalk({
            id: "af047e8e31acd34ddf0417c7ec6acfa3",
            repo: "gitalk",
            owner: "codeflysafe",
            clientID: "48d1eb3658ef4923896f",
            clientSecret: "d619d46a6c36396cab7dc0190ad1630bd3982b34",
            admin: ["codeflysafe"],
            createIssueManually: false,
            distractionFreeMode: false,
            perPage: 10,
            pagerDirection: "last",
            
            
            enableHotKey: true,
            
        })
        gitalk.render('comment-container')</script></div></div></div><!--!--><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="魔法沉思录" height="28"></a><p class="is-size-7"><span>&copy; 2022 sjhuang</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>