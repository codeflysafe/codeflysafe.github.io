{"pages":[{"title":"about","text":"","link":"/about/index.html"}],"posts":[{"title":"about","text":"","link":"/2021/07/06/about/"},{"title":"find-majority-element-lcci","text":"数组中占比超过一半的元素称之为主要元素。给你一个 整数 数组，找出其中的主要元素。若没有，返回 -1 。请设计时间复杂度为 O(N) 、空间复杂度为 O(1) 的解决方案。 示例 1： 12输入：[1,2,5,9,5,9,5,5,5]输出：5 示例 2： 输入：[3,2]输出：-1 示例 3： 12输入：[2,2,1,1,1,2,2]输出：2 来源：力扣（LeetCode）链接：https://leetcode-cn.com/problems/find-majority-element-lcci著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。 核心思想假设存在元素a的数量超过一半,则将所有不同的元素相互抵消，剩余的那一个肯定是a.如： 12[2,2,1,1,1,2,2]-&gt; [2] 当不存在这样的元素时，也可能最后剩余的元素不符合条件，这时候就需要统计剩余元素的个数，然后判断是否超过总数的一半即可。如： 12[1,2,3]-&gt; [3] Code123456789101112131415161718192021222324class Solution {public: int majorityElement(vector&lt;int&gt;&amp; nums) { int num = nums[0], count = 1, n = nums.size(); for(int i =1; i&lt;n; i++){ if(count == 0){ num = nums[i]; count = 1; }else{ if(nums[i] != num){ count --; }else count++; } } if(count == 0) return -1; else{ count = 0; for(int nu: nums){ if(nu == num) count++; } return count*2 &gt;= n ? num:-1; } }};","link":"/2021/07/09/find-majority-element-lcci/"},{"title":"group-anagrams-lcci [计数统计]","text":"编写一种方法，对字符串数组进行排序，将所有变位词组合在一起。变位词是指字母相同，但排列不同的字符串。 注意：本题相对原题稍作修改 示例: 123456789输入: [&quot;eat&quot;, &quot;tea&quot;, &quot;tan&quot;, &quot;ate&quot;, &quot;nat&quot;, &quot;bat&quot;],输出:[ [&quot;ate&quot;,&quot;eat&quot;,&quot;tea&quot;], [&quot;nat&quot;,&quot;tan&quot;], [&quot;bat&quot;]] 说明： 所有输入均为小写字母。 不考虑答案输出的顺序。 来源：力扣（LeetCode）链接：https://leetcode-cn.com/problems/group-anagrams-lcci著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。 题解由于只有小写字母，只需要申请一个长度为26的数组统计每一个字符串的字母数量即可（类似于hashcode的构造思想）。这样所有的变位词可以映射到相同的数组，使用map，将数组（或者转为字符串）作为key，value 即是对应的字符数组。 Code12345678910111213141516171819202122232425262728293031class Solution {public: string hashcode(vector&lt;int&gt; chs){ string ans = &quot;&quot;; for(int ch: chs){ ans.append(to_string(ch) + &quot;_&quot;); } return ans; } vector&lt;vector&lt;string&gt;&gt; groupAnagrams(vector&lt;string&gt;&amp; strs) { // hash 编码 // eat tea tan ate nat bat unordered_map&lt;string,vector&lt;string&gt;&gt; containers; for(string str: strs){ vector&lt;int&gt; counts(26,0); for(char c: str){ counts[c-'a'] ++; } containers[hashcode(counts)].emplace_back(str); } vector&lt;vector&lt;string&gt;&gt; res; for(auto &amp;p: containers){ res.push_back(p.second); } return res; }};","link":"/2021/07/18/group-anagrams-lcci/"},{"title":"minimize-maximum-pair-sum-in-array","text":"一个数对 (a,b) 的 数对和 等于 a + b 。最大数对和 是一个数对数组中最大的 数对和 。 比方说，如果我们有数对 (1,5) ，(2,3) 和 (4,4)，最大数对和 为 max(1+5, 2+3, 4+4) = max(6, 5, 8) = 8 。给你一个长度为 偶数 n 的数组 nums ，请你将 nums 中的元素分成 n / 2 个数对，使得： nums 中每个元素 恰好 在 一个 数对中，且最大数对和 的值 最小 。请你在最优数对划分的方案下，返回最小的 最大数对和 。 示例 1： 12345输入：nums = [3,5,2,3]输出：7解释：数组中的元素可以分为数对 (3,3) 和 (5,2) 。最大数对和为 max(3+3, 5+2) = max(6, 7) = 7 。 示例 2： 12345输入：nums = [3,5,4,2,4,6]输出：8解释：数组中的元素可以分为数对 (3,5)，(4,4) 和 (6,2) 。最大数对和为 max(3+5, 4+4, 6+2) = max(8, 8, 8) = 8 。 提示： n == nums.length2 &lt;= n &lt;= 105n 是 偶数 。1 &lt;= nums[i] &lt;= 105 来源：力扣（LeetCode）链接：https://leetcode-cn.com/problems/minimize-maximum-pair-sum-in-array著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。 Solution贪心算法，假设序列 [a1,a2,a3,a4,a5,a6],且 a1 &lt;= a2 &lt;= a3 &lt;= .. &lt;= a6, 则必有最小的 最大数对和只能是(a1,a6),(a2,a5),(a3,a4)数对之和的最大值。 证明简单证明一下，假设存在四个数 a &lt; b &lt; c &lt; d, 则符合条件的数为 max(a+d,b+c)。反证法: 若 d 不与 a搭配,与其它任意搭配，都存在结果为 d + ?. Code12345678910111213class Solution {public: int minPairSum(vector&lt;int&gt;&amp; nums) { sort(nums.begin(),nums.end()); int ans = 0, n = nums.size() - 1, i = 0; while(i &lt; n){ ans = max(ans,nums[i]+nums[n]); i++; n--; } return ans; }};","link":"/2021/07/20/minimize-maximum-pair-sum-in-array/"},{"title":"template","text":"常见的一些latex、hexo bug 或者其它的使用说明书. 1. latexlatex syntax 1.1 行内样式 inline synatx$\\hat{x}_{k}=\\hat{x}_{k}^{-}+K_{t}\\left(y_{k}\\right)$ 1.2 equation \\begin{align} & \\arg\\min_{w,b} \\frac{1}{2m}\\sum_{i=1}^m (\\hat{y_i} - y_i)^2 \\\\ & s.t \\quad \\hat{y_i}(w^x_i + b) >= 0\\\\ \\end{align} 1.3 矩阵 $$\\begin{bmatrix} {a_{11}}&{a_{12}}&{\\cdots}&{a_{1n}}\\\\ {a_{21}}&{a_{22}}&{\\cdots}&{a_{2n}}\\\\ {\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}\\\\ {a_{m1}}&{a_{m2}}&{\\cdots}&{a_{mn}}\\\\ \\end{bmatrix}$$ 1.4 分母$$P(A_i \\mid B) = \\frac{P(B\\mid A)P(A_i)}{\\sum_{j=1}^{n}P(A_j)P(B \\mid A_j)}$$ 1.5 常见的希腊字符 希腊字符 latex 大写 latex｜ $\\alpha$ \\alpha $\\lambda$ \\lambda $\\Lambda$ \\Lambda $\\delta$ \\delta $\\Delta$ \\Delta $\\beta$ \\beta $\\sigma$ \\sigma $\\Sigma$ \\Sigma $\\theta$ \\theta $\\Theta$ \\Theta $\\epsilon$ \\epsilon To be continue","link":"/2021/07/06/template/"},{"title":"time-based-key-value-store","text":"创建一个基于时间的键值存储类 TimeMap，它支持下面两个操作： set(string key, string value, int timestamp) 存储键 key、值 value，以及给定的时间戳 timestamp。 get(string key, int timestamp) 返回先前调用 set(key, value, timestamp_prev) 所存储的值，其中 timestamp_prev &lt;= timestamp。如果有多个这样的值，则返回对应最大的 timestamp_prev 的那个值。如果没有值，则返回空字符串（””）。 示例 1：12345678910输入：inputs = [&quot;TimeMap&quot;,&quot;set&quot;,&quot;get&quot;,&quot;get&quot;,&quot;set&quot;,&quot;get&quot;,&quot;get&quot;], inputs = [[],[&quot;foo&quot;,&quot;bar&quot;,1],[&quot;foo&quot;,1],[&quot;foo&quot;,3],[&quot;foo&quot;,&quot;bar2&quot;,4],[&quot;foo&quot;,4],[&quot;foo&quot;,5]]输出：[null,null,&quot;bar&quot;,&quot;bar&quot;,null,&quot;bar2&quot;,&quot;bar2&quot;]解释： TimeMap kv; kv.set(&quot;foo&quot;, &quot;bar&quot;, 1); // 存储键 &quot;foo&quot; 和值 &quot;bar&quot; 以及时间戳 timestamp = 1 kv.get(&quot;foo&quot;, 1); // 输出 &quot;bar&quot; kv.get(&quot;foo&quot;, 3); // 输出 &quot;bar&quot; 因为在时间戳 3 和时间戳 2 处没有对应 &quot;foo&quot; 的值，所以唯一的值位于时间戳 1 处（即 &quot;bar&quot;） kv.set(&quot;foo&quot;, &quot;bar2&quot;, 4); kv.get(&quot;foo&quot;, 4); // 输出 &quot;bar2&quot; kv.get(&quot;foo&quot;, 5); // 输出 &quot;bar2&quot; 示例 2：12输入：inputs = [&quot;TimeMap&quot;,&quot;set&quot;,&quot;set&quot;,&quot;get&quot;,&quot;get&quot;,&quot;get&quot;,&quot;get&quot;,&quot;get&quot;], inputs = [[],[&quot;love&quot;,&quot;high&quot;,10],[&quot;love&quot;,&quot;low&quot;,20],[&quot;love&quot;,5],[&quot;love&quot;,10],[&quot;love&quot;,15],[&quot;love&quot;,20],[&quot;love&quot;,25]]输出：[null,null,null,&quot;&quot;,&quot;high&quot;,&quot;high&quot;,&quot;low&quot;,&quot;low&quot;] 提示： 所有的键/值字符串都是小写的。 所有的键/值字符串长度都在 [1, 100] 范围内。 所有 TimeMap.set 操作中的时间戳 timestamps 都是严格递增的。 1 &lt;= timestamp &lt;= 10^7 TimeMap.set 和 TimeMap.get 函数在每个测试用例中将（组合）调用总计 120000 次。 来源：力扣（LeetCode）链接：https://leetcode-cn.com/problems/time-based-key-value-store著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。 Code123456789101112131415161718192021222324252627282930313233class TimeMap {public: using P = pair&lt;int,string&gt;; unordered_map&lt;string,vector&lt;P&gt;&gt; container; /** Initialize your data structure here. */ TimeMap() { // unordered_map&lt;string,priority_queue&lt;Point&gt;&gt; container; } void set(string key, string value, int timestamp) { container[key].emplace_back(P(timestamp,value)); } string get(string key, int timestamp) { auto &amp;pairs = container[key]; auto it = upper_bound(pairs.begin(),pairs.end(),P(timestamp,string({127}))); if(it != pairs.begin()){ return (it -1)-&gt;second; } return &quot;&quot;; }};/** * Your TimeMap object will be instantiated and called as such: * TimeMap* obj = new TimeMap(); * obj-&gt;set(key,value,timestamp); * string param_2 = obj-&gt;get(key,timestamp); */ More Info1. Pair&lt;int,string&gt;2. Vector&lt;Pair&lt;int,string&gt;&gt;","link":"/2021/07/10/time-based-key-value-store/"},{"title":"applications_of_parallel_computers","text":"cs267 是cmu的一门关于并行算法的公开课下面是它的一个详细的课程表 CS267 Master Schedule Sp21 并行算法","link":"/2021/09/03/applications-of-parallel-computers/"},{"title":"感知机","text":"感知机(Perceptron), 是一种二分类的线性分类器。 其输入是一个待分类的实例，输出为该实例对应的类型。 根据模型、策略和算法三部曲，来说： 1. 模型由于是一个分类器，其函数模型为： $$ {\\begin{aligned} &z = w^Tx + b \\\\ &f(x) = sign(z) = \\begin{cases} 1, & \\text{if z > 0} \\\\ -1, & \\text{if z < 0} \\end{cases} \\end{aligned}} $$ 2. 策略分类问题的误差函数，可以采用误分类个数之和 $$Loss(w,b) = \\sum_i I(y_i,f(x_i)) \\\\ I(y_i,f(x_i)) = \\begin{cases} 1, &\\text{if $y_i\\neq f(x_i)$ } \\\\ 0, &\\text{if $y_i = f(x_i)$} \\\\ \\end{cases}$$ 但是，该函数无法使用梯度下降法来进行求解。可以考虑一下，每个点到该分类器超平点的距离。 2.1 损失函数假设存在一个超平面 S: $w^Tx + b = 0$ ，将特征划分为正、负两个超平面。超平面 S，其中 w是法向量，b为截距，如下图绿线 此时，我们采用误分类点到超平面的距离之和来作为损失函数。向量空间的任意一点 x 到超平面S的距离为 $ dis = \\frac{1}{||w||}|w^Tx_i + b| \\\\$ 由于， 对于误分类的点有： $$ \\begin{cases} w^Tx + b < 0,&\\text{if $y = 1$} \\\\ w^Tx + b > 0,&\\text{if $y = -1$} \\\\ \\end{cases} \\\\ -y(w^Tx + b) > 0 , x\\subset M \\\\ $$ 因此损失函数为： $L(w,b) = \\sum_i^M -y_i(w^Tx_i + b) \\qquad (3) \\\\$ > 这里省略掉了 ||w|| 或者说是加了约束条件 ||w|| = 1, 思考为什么可以？ > 答案见附录 B-1 明显是一个无约束的线性优化问题，转化为优化问题为： $\\arg\\min_{w,b} L(w,b) \\\\ s.t \\rightarrow x\\in M \\qquad (4)$ M 是所有误分类的集合，即： $$ M = {x| y(w^Tx + b )","link":"/2021/03/28/%E6%84%9F%E7%9F%A5%E6%9C%BA/"},{"title":"朴素贝叶斯","text":"朴素贝叶斯（naive bayes）是基于贝叶斯理论的分类器，它以变量的各个特征之间相互独立为前提，利用条件概率来最大化后验概率或者最小化期望风险，来实现判别其类别。 1. 基础理论根据条件概率或者贝叶斯理论有： $$P(Y = c_k/ X = x) = \\frac{P(Y = c_k, X = x)}{P(X = x)} = \\frac{ P(X = x/ Y = c_k)P(Y = c_k)}{\\sum_k^K P(X = x/ Y = c_k)P(Y = c_k) } \\$$ 其中 Y 为类别集合 {${c_1,c_2,…c_k}$};X 为输入空间的 n 维向量集合 {${x_i,x_2,…x_n}$} 。 目标为对于给定的一个输入向量，通过模型能够获取其类别（这里取给定x情况下，取概率最大的类别作为最终类别），即 $$Given,x \\in X \\y = f(x) = \\arg\\max_{ck}P(Y = c_k/X = x), k =1,2,3…K$$求解概率 $P(Y = c_k/X = x)$，可以有贝叶斯理论可知，先求解$P(Y = c_k, X = x), P(X = x)$。 1.1 求解$P(Y = c_k,X = x)$这里假设了，对于变量$$x = [x^{(1)},…,x^{(m)}]$$的不同特征之间相互独立，因此: $$\\begin{aligned}P(Y=c_k,X=x) &amp;= P(Y=c_k,X^{(1)} \\ &amp;= x^{(1)},…,X^{(m)} = x^{(m)} ) \\ &amp;= \\prod_{i}^m P(Y=c_k,X^{(i)} = x^{(i)}) \\ &amp;= P(Y = c_k) \\prod_{i}^m P(X^{(i)}= x^{(i)}/ Y = c_k )\\end{aligned}$$ 1.2 求解 $P(X = x)$有贝叶斯可知：$$\\begin{aligned}P(X = x) &amp;= \\sum_k^K P(X = x/ Y = c_k)P(Y = c_k) \\&amp;= \\sum_k^K P(Y=c_k) \\prod_i^m P(X^{(i)} = x^{(i)}/ Y = c_k )\\end{aligned}$$ 但是 $P(Y = c_k ) , , P(X^{(i)} = x^{(i)}/ Y = c_k )$ 如何确定呢？ 2. 最大化后验概率或者最小化期望风险后验概率最大化即：$\\max P(Y=c_k/X=x)$ 假设误差损失函数：$$\\begin{aligned}L(Y, f(X)) = \\begin{cases} 0 &amp;, Y = f(X) \\ 1 &amp;, Y \\neq f(X)\\\\end{cases}\\end{aligned}$$则模型目标为最小化期望风险即： $$\\begin{aligned}R_{exp}(f) &amp;= E[L(Y,f(X)]\\&amp;= E_X\\sum_k^KP(c_k/X) L(c_k,f(X)) \\&amp;= E_X\\sum_k^{K}L(y,c_k)P(c_k/X=x) \\&amp;= E_X\\sum_k^{K}P(y \\neq c_k/X=x) \\qquad (1)\\end{aligned}$$ 最小化式子(1)，只需要对 X=x 逐个最小化即可。 $$\\begin{aligned}f(x) &amp;= \\arg\\min P(y\\neq c_k/X=x) \\&amp;= \\arg\\min (1 - P(y = c_k/X= x)) \\&amp;= \\arg \\max P(y=c_k/X=x)\\end{aligned}$$ 因此，两者是等价的 3. 贝叶斯的参数估计前面提到问题：但是 $P(Y = c_k ) , , P(X^{(i)} = x^{(i)}/ Y = c_k )$ 如何确定呢？对于离散变量，一个直观的概念是使用其统计学方法来近似估计概率。对于连续变量，采用一个概率分布拟合其变量分布，如高斯分布 3.1 离散型变量估计令 $\\theta = P(Y = c_k)$ 参与极大似然概率来估计 $\\theta$, 设变量个数为n,其中类别为$c_k$的有t个，则有： $$\\begin{aligned}&amp;L(\\theta) = (\\theta)^{t}(1 - \\theta)^{(n-t)} \\&amp;LnL(\\theta) = tLn(\\theta) + (n-t)Ln(1-\\theta) \\&amp;\\frac{dLnL(\\theta)}{d\\theta} = \\frac{t}{\\theta} + \\frac{-(n-t)}{1 - \\theta} = 0 \\&amp;\\hat\\theta = \\frac{t}{n}\\end{aligned}$$即： $P(Y=c_k) = \\frac{t}{n}$同理可知：$P(X^{(i)} = x^{(i)}/ Y = c_k ) = \\frac{t_{ni}}{n_k}$, 其中 $n_k$ 为类别为 $c_k$的数量，$t_{ni}$为在类别为$c_k$的数据中，$X^{(i)} = x^{(i)}$的数量由于，对于任意类别 $P(X = x)$ 的值是相同的，因此可以省略… 3.2 连续性变量估计如果X的各个特征是连续的，这是可以采用某种具体的分布函数来拟合X各个特征的分布，比如采用 Normal Distribution 等等。通常在确定分布函数类型之前，先对数据进行可视化处理，观察其数字特征等，在确定采用哪种分布函数来拟合。 以高斯分布为例，此时 $X_i \\sim (\\mu\\sigma)$： $$\\begin{aligned}&amp;\\mu_i = Ex_i = \\bar{x} \\&amp;\\sigma_i^2 = E((X_i-\\mu_i)^2)\\end{aligned}$$ 这里其实要使用最大似然来估计一下变量 $\\mu$和 $\\sigma$ $$L(\\mu,\\sigma) = \\prod_i^mf(x_i)\\LnL = \\sum_i^m\\ln{f(x_i)}$$对上式分别对 $\\mu$和 $\\sigma$求偏导数，并令其为0即可 更简化，由于变量的各个维度都是独立的，因此可以联合分布为各个分布函数的乘积，如果都是符合高斯分布，即有： $$\\begin{aligned}f(x_1,x_2…x_m) &amp;= \\prod_i^mf(x_i) \\&amp;= \\prod_i^m \\frac{1}{(2\\pi\\sigma_i^2)^{1/2}} \\exp{\\frac{-(x_i - \\mu_i)^2}{2\\sigma_i^2}} \\&amp;= \\frac{1}{(2\\pi)^m||\\Sigma||}^{1/2} \\exp{-\\frac{1}{2}(X - \\mu)^T\\Sigma^{-1}(X - \\mu)} \\\\end{aligned}$$ 只需要计算 X 的方差和均值即可。 另外，如果 X 得来源于一个公共得资源，可以假设各个特征得方差均相同 if we have reason to believe that noise in the observed _Xi _comes from a common source, then we might further assume that all of the σ_ik _are identical, regardless of the attribute _i _or class _k _(see the homework exercise on this issue). 贝叶斯模型存在的问题 无偏差的贝叶斯模型是不现实的？ 思考一下，如果变量的特征一个离散型的，一个是连续性的，如何设计和求解模型呢？逻辑回归模型 2. 从朴素贝叶斯到逻辑回归首先在了解逻辑回归模型的之前，我们以二分类为例，使用朴素贝叶斯的方法来判别分类。 $$\\begin{aligned}P(Y = c_1/X = x) &amp;= \\frac{P(Y = c_1)P(X =x/ Y =c_1)}{P(X = x)} \\&amp;=\\frac{P(Y = c_1) P(X =x/Y=c_1)}{\\sum_i^2 P(Y = c_1) P(X =x/Y=c_1)} \\&amp;=\\frac{1}{1 + \\frac{P(Y=c_2)P(X =x/Y=c_2)}{P(Y =c_1)P(X =x/Y=c_1)}} , (3) \\\\end{aligned}$$ 由于 $P(X = x/ Y= c_1 ) = f(x) = \\frac{1}{(2\\pi)^m||\\Sigma||}^{1/2} \\exp{-\\frac{1}{2}(x - \\mu)^T\\Sigma^{-1}(x - \\mu)}$ 带入到式（3） 中，可得： $P(Y=c_1/X= x) = \\frac{1}{1 + \\exp(-z)} = \\sigma(z)$ 这就是 Sigmoid Function $z = ln(\\frac{P(c1)P(x/c1)}{P(c2)P(x/c2)}) = ln\\frac{P(c1)}{P(c2)} + ln\\frac{P(x/c1)}{P(x/c2)}$ 其中 $P(Y=c_k) = \\frac{t}{n}$，可知 $ln\\frac{P(c1)}{P(c2)}$ 是常数 这里假设 $\\Sigma_1 = \\Sigma_2 = \\Sigma$ $$ln\\frac{P(x/c1)}{P(x/c2)} = \\frac{1}{2}(x-\\mu_1)^T\\Sigma^{-1}(x-\\mu_1) - \\frac{1}{2}(x-\\mu_2)^T\\Sigma^{-1}(x-\\mu_2) \\= (\\mu_1-\\mu_2)^T\\Sigma^{-1}x - \\frac{1}{2}( \\mu_1^T\\Sigma^{-1}\\mu_1 - \\mu_2^T\\Sigma^{-1}\\mu_2 )$$ 令 $b = ln\\frac{P(c1)}{P(c2)} + - \\frac{1}{2}( \\mu_1^T\\Sigma^{-1}\\mu_1 - \\mu_2^T\\Sigma^{-1}\\mu_2 )$, $w^T = (\\mu_1-\\mu_2)^T\\Sigma^{-1}$ 则有 $z = w^Tx + b$ 这里其实是有一个差异，因为 w 和 b 之间是有某种依赖关系的，但是如果只看作是 w 和 b变量，则w和b之间相互独立，这样子大大减少了训练的参数，这也就是逻辑回归。 3. 从回归模型到逻辑回归 还是以二分类为例 回归模型为 $y(x) = W^Tx + b$ ， 这里我们想要找到一种模型使：$P_{w,b}(c/x)$ $$\\begin{cases} output , c1 ,, P_{w,b}(c/x) &gt;= 0.5 \\ otherwise ,, output ,c2\\end{cases}$$ 这里我们考虑是有 sigmoid funcion $\\sigma(z) = \\frac{1}{1 + \\exp{(-z)}}$其取值范围为 (0 , 1 )，自变量取值为$( -\\infty，+\\infty)$, $z = w^Tx + b$其图形如下： 其实就是使用 sigmoid function 来替代 p(y/x)的概率，因此这种模型与前文看到模型不同。前文（贝叶斯）是使用间接计算，即由 P(y) 和 P(x/y) 来间接求的 p(y/c)的概率，这种模型叫做生成模型（Generative model）。而逻辑回归，是直接采用一个函数来模拟 p(y/x)的分布，成为判别模型(discriminative model) 3.1 策略逻辑回归的损失函数可以分为两种，一种为 maximum likelihood estimation ，另外一种为 the least squares , 一般采用最大似然概率。 想一下为什么？ 其损失函数为：假设 $z_1,z_2 … z_m 为 c_1, z_{m+1}… z_n 为 c_2$则$$L(w,b) = \\prod_i^{m} \\sigma(z_i) \\prod_{i = m+1}^n (1- \\sigma(z_i))\\\\ln{L} = \\sum_i^m \\ln{(\\sigma(z_i))} + \\sum_{m+1}^{n} \\ln{(\\sigma(z_i))} \\= E_{x\\sim c_1} \\ln{\\sigma(z)} + E_{x\\sim c_2} \\ln{\\sigma(z)}\\= \\sum_i^n\\hat{y_i}\\ln{\\sigma(z_i)} + (1 - \\hat{y_i} )ln{(1 - \\sigma(z_i))}$$ 对于每一个给定的x, 使 $f = \\sigma(z_i)ln{\\sigma(z_i)} + (1 - \\sigma(z_i) )ln{(1 - \\sigma(z_i))}$ 最大。这里直接对w,b 求偏导数，使用随机梯度下降法即可。 $$\\frac{df}{dw} = \\frac{df}{dz} \\frac{dz}{dw} \\ \\frac{d\\sigma}{dz} = \\frac{\\exp{(-z)}}{(1 + \\exp{(-z)})^2} = \\sigma(1 - \\sigma) \\\\frac{dz}{dw} = x\\\\frac{df}{dw} = (y(1-\\sigma) - (1-y)\\sigma)x = (y - \\sigma)x \\\\frac{df}{db} = (y(1-\\sigma) - (1-y)\\sigma)x = y - \\sigma$$ 4. 思考一下 朴素贝叶斯法和逻辑回归做分类问题的时候，有什么区别？各自有什么优势？ 逻辑回归的损失函数使用 cross entropy 和 square error 用什么优劣？ 参考 《统计学习方法》 李航， chapter 4 Machine Learning , Tom M. Mitchell , chapter 3 Classification 李宏毅","link":"/2020/09/22/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%92%8C%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"},{"title":"多层感知机与BP算法","text":"前面讲解 感知机 , 说明其有一个很大的不足之处，即它无法求解非线性问题或者异或问题。后面就发展出了多层感知机（deep feebforward network or multilayer perceptron），以及求解使用的BP算法。这也是深度学习的基础。 http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2016/Lecture/DL%20(v2).pdf 1. 多层感知机 1.1 Neuron我们将逻辑回归抽象成一个neural network的一个“Neuron”，而多层感知机即是将这些神经元连接在一起。对于深度学习网络来说，它也是有一个个神经元组成，但是它可以有各个不同的网络结构（网络堆叠）。 1.2 Multilayer Perceptron多层感知机,其中一个小白色长方形（f）为一个一个”Neuron”，每一层可以有多个 Neruon, 而多层感知机可以有很多层。其中灰色部分为隐含层（Hidden Layer），绿色为输入层(Input Layer),黄色部分为输出层 (Output Layer) 。蓝色线可以看作是一个权重（w), 而 f 代表激活函数（activate function）。 目前激活函数有很多种，常用的有 Sigmoid、Relu 、Tanh、Leaky Relu、Softmax 等。 多层感知机是一种监督模型，它可以解决分类和回归问题（输出层的结构有所不同）。一般在解决分类问题时，它会在输出层前增加一层 softmax 处理，来模拟各个分类的概率。下面一分类问题为例子，来了解它的整个运作过程。 1.2.1 数据流动 数据是如何在感知机内流动的？ 本质上，多层感知机是一个个矩阵的乘法运算，它的数据流是从前向后流动的，即 Feedforward 如，输入为 $x = [x_1, x_2,…,x_m]$, 其中$x \\in R^n$, 即单个x存在m个特征。对于隐含层第元素有： $w_{ji}^L$代表第L层的j个z值与第L-1层的第i个输入之间的权重 $b_j^L$代表第L层的偏移量 $a_i^L$代表第L层输出的第i的元素，同时是第L+1的输入的第i个元素 f为激活函数 $z_{j}^L = \\sum_i w_{ji}^La_i^{L-1} + b_j^L$ $a_j^L = \\sigma(z_j^L)$ 对于输入层有： $a_i^0 = x_i$ 对于输出层有（这里不考虑softmax）： $y_i = a_i^L$ 数据流通为： $$ \\begin{aligned} z^L &= \\begin{bmatrix} w_{00}^L & w_{01}^L& ... &w_{0i}^L\\\\ w_{10}^L & w_{11}^L& ... &w_{1i}^L\\\\ ... & ...& ... & ...\\\\ w_{j0}^L & w_{j1}^L& ... &w_{ji}^L\\\\ \\end{bmatrix} \\begin{bmatrix} a_0^{L-1}\\\\ a_1^{L-1}\\\\ ...\\\\ a_i^{L-1}\\\\ \\end{bmatrix} + \\begin{bmatrix} b_0^{L}\\\\ b_1^{L}\\\\ ...\\\\ b_j^{L}\\\\ \\end{bmatrix} \\\\ & = w^La^{L-1} + b^L \\\\ a^L &= f(z^L) = f(w^L a^{L-1} + b^L ) \\end{aligned}\\tag{2} $$ 其中 $w^L$ 为 jxi 得 matrix, j为第L层的输出Dimensions，i为第``L层的输入的Dimensions。根据公式1, 2,可以推出输出为: $$ \\begin{aligned} y &= a^L \\\\ &= f(w^La^{L-1} + b^L ) = f(w^Lf(w^{L-1}a^{L-2}+ b^{L-1})+ b^L ) \\\\ &= ... \\\\ &= f(w^Lf(w^{L-1}f(...f(...f(w^1a^0 + b^0) + b^{l} ) + b^{L-1}) + b^L ) \\\\ \\end{aligned}\\tag{3} $$ 1.2.2 损失函数由于多层感知机可以处理分类和回归问题，所有其损失函数也分为两类，下面以分类问题为例。可以看作是多分类问题，这是可以采取最大似然概率来作为其损失函数（交叉熵）。做分类问题时，需要在输出层之前增加一个 softmax 层。 Tips 这里看出多层感知机其实是一个 discriminative model 。 假设共有m个类别 $(c_1,…c_m)$，$\\theta$ 为所有的参数，则 Loss Function 为： $$ \\begin{aligned} L(\\theta) &= \\sum_i^m {\\sum_{x^j\\in c_i}\\ln(y_i^j) + \\sum_{x^j\\notin c_i}\\ln(1 - y_i^j) } \\\\ & = \\sum_i^m { E_{x^j\\sim c_i} \\ln{y_i^j} + E_{x^j\\sim \\bar{c_i}} \\ln{(1 - y_i^j) }} \\\\ &= \\sum_i^m\\sum_j^{n_{c_i}} \\{\\hat{y}_{i}^j\\ln{y_i^j} + (1- \\hat{y}_{i}^j) \\ln{(1 - y_i^j)} \\} \\\\ &= \\sum_i^n \\sum_j^m \\hat{y}_j\\ln{y_j} + (1-\\hat{y}_j )\\ln{(1 - y_j)} \\end{aligned} \\tag{4} $$ 或者使用 $$ \\begin{aligned} L(\\theta) &= \\sum_i^m {\\sum_{x^j \\in c_i}\\ln(y_i^j)} \\\\ & = \\sum_i^m {E_{x^j\\sim c_i} \\ln{y_i^j} } \\\\ &= \\sum_i^m\\sum_j^{n_{c_i}} \\hat{y}_{i}^j \\ln{y_i^j} \\\\ &= \\sum_i^n \\sum_j^m \\hat{y}_j\\ln{y_j} \\end{aligned} \\tag{5} $$ 其中 $\\hat{y} = \\begin{bmatrix} 0,0,.,1,.,0 \\end{bmatrix}^T$ ,one-shot vector 的样式 Tips: 如果是回归模型的话，无需添加 softmax 层，而且可以使用 mean square errors 作为损失函数 Thinking 式4和式5在使用过程中，有什么区别呢？那个效果更好呢？ 了解了损失函数之后，那么如何来寻找损失函数的极大值呢？或者是如何解决这个优化问题呢？这里就引出了BackPropagation, 即BP算法 2. BP算法由式子（5） 可知, 使用SGD来解该优化问题，在给定x的情况下, 令 c 为损失函数。首先对隐含层某个参数求偏导数，如 $w_{ji}^L$ $$\\frac{dc}{dw_{ji}^L} = \\frac{dc}{da_j^L}\\frac{da_j^L}{dw_{ji}^L} \\tag{6}$$ 其中 $\\frac{da_j^L}{dw_{ji}^L}$ $$\\frac{da_j^L}{dw_{ji}^L} = \\frac{da_j^L}{dz_j^L}\\frac{dz_j^L}{dw_{ji}^L} = f’(z_j^L)\\frac{dz_j^L}{dw_{ji}^L} = f’(z_j^L)a_i^{L-1} \\tag{7}$$ 其中$\\frac{dc}{da_j^L}$ $$\\begin{aligned}\\delta_j^L &amp;= \\frac{dc}{da_j^L} \\&amp;= \\sum_k^K \\frac{dc}{dz_k^{L+1}}\\frac{dz_k^{L+1}}{da_j^L}\\&amp; = \\sum_k^K\\frac{dc}{dz_k^{L+1}}w_{kj}^{L+1} \\&amp;= \\sum_k^K\\frac{dc}{da_k^{L+1}}\\frac{da_k^{L+1}}{dz_k^{L+1}}w_{kj}^{L+1} \\&amp;= \\sum_k^K \\delta_k^{L+1}f’(z_k^{L+1})w_{kj}^{L+1}\\end{aligned}\\tag{8}$$ 令 $\\delta^L = \\begin{bmatrix} \\delta_1^L, …, \\delta_J^L \\end{bmatrix}^T$, $\\sigma ^L = \\begin{bmatrix} f’(z_1^L), …, f’(z_J^L) \\end{bmatrix}^T$, $w^L = \\begin{bmatrix} w_1^L,…,w_J^L\\end{bmatrix}$ 则 $$\\begin{aligned}\\delta_j^L &amp;= \\sum_k^K \\delta_k^{L+1}f’(z_k^{L+1})w_{kj}^{L+1} \\&amp;= \\langle{(\\delta^{L+1} \\odot \\sigma^L),w_j^{L+1}} \\rangle\\end{aligned} \\tag{9}$$ 由于$\\delta^L = \\begin{bmatrix} \\delta_1^L, …, \\delta_J^L \\end{bmatrix}^T$, 故： $$\\delta^L =\\begin{bmatrix} \\delta_1^L, …, \\delta_J^L \\end{bmatrix}^T = (w^{L+1})^T (\\delta^{L+1} \\odot \\sigma^L) \\tag{10}$$ 因此找到了$\\delta^L$和$\\delta^{L+1}$ 之间的关系，一旦得到了$\\delta^{L+1}$就可以求出$\\delta^L$。令 $\\frac{dc}{dw_j^L} =\\begin{bmatrix}\\frac{dc}{dw_{j1}^L},…,\\frac{dc}{dw_{jI}^L}\\end{bmatrix}^T$又因为：$\\frac{dc}{dw_{ji}^L} = f’(z_j^L)a_i^{L-1}\\delta_j^L$故有 $$\\frac{dc}{dw_{j}^L} = \\sigma^L \\odot a^{L-1} \\odot \\delta^L \\tag{11}$$在输出层有：$\\frac{dc}{dy}$， 这个要根据具体的损失函数来确定 $\\frac{dy_i}{da_i^L}$ 这个如果没有 softmax 层或者说是回归问题的话，值因为1 Tips 如果是softmax的话，应该怎么做？ 因此，对最后一个隐含层的输出有 $$\\begin{aligned} \\delta &amp;= \\frac{dc}{da} = \\frac{dc}{dy}\\frac{dy}{da}\\end{aligned} \\tag{12}$$ 然后一次向前求解，即可求出对所有参数的倒数。 对 $b_j^L$求导数时： $\\frac{dc}{db_{j}^L} = \\frac{dc}{da_j^L}\\frac{da_j^L}{db_j^L} = \\delta_j^Lf’(z_j^L) \\\\frac{da_j^L}{db_{j}^L} = \\frac{da_j^L}{dz_j^L}\\frac{dz_j^L}{db_{j}^L} = f’(z_j^L)$因此 $$\\frac{dc}{db^L} = \\delta^L \\odot \\sigma^L$$","link":"/2020/09/23/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%E4%B8%8EBP%E7%AE%97%E6%B3%95/"},{"title":"线性回归","text":"1. 线性回归模型给定数据集 $D = { (x_1,y_1),(x_2,y_2) …,(x_n,y_n)}$, 其中 $x_i = (x_i^1, x_i^2, … x_i^m)$。 线性回归是试图使用一个线性模型来尽可能的预测实际数值的输出标记。 假设，使用函数: $$ \\begin{aligned} y = w^Tx + b \\\\ w = \\begin{bmatrix} w^1 \\\\ ...\\\\ w^m \\end{bmatrix} \\end{aligned} $$ b 是一个 scalar 。使用均方误差作为回归模型的性能度量，则可以转化为一个无约束的优化问题： $\\min_{w,b} L(w,b) = \\frac{1}{2n}\\sum_i^n(y_i - \\hat{y}_i)^2$ 解决这个优化问题，可以求导并令其偏导数为0，即可。下面是分别对 w , b 求导的结果 $$\\begin{aligned} \\frac{dL}{dw} &amp;= \\frac{1}{2n}\\sum_i^n 2(y_i-\\hat{y}_i)\\frac{dy_i}{dw} \\&amp;= \\frac{1}{n}\\sum_i^n(y_i-\\hat{y}_i)x_i \\qquad (1)\\\\frac{dL}{db} &amp;= \\frac{1}{n}\\sum_i^n(y_i - \\hat{y}_i) \\qquad (2)\\end{aligned}$$ 通常会把(1),(2) 式写成矩阵的形式，即: $$\\begin{aligned}\\frac{dL}{dw}&amp;= \\frac{1}{n}X^T(Xw - \\hat{Y}) \\qquad (3) \\\\frac{dL}{db}&amp;= \\frac{1}{n}I(Xw -\\hat{Y}) \\qquad (4) \\\\end{aligned}$$ 其中 $X=\\begin{bmatrix}x_1 \\ x_2 \\ … \\ x_n \\end{bmatrix}$, $\\hat{Y}=\\begin{bmatrix}\\hat{y}_1 \\ \\hat{y}_2 \\ … \\ \\hat{y}_n\\end{bmatrix}$, $I=\\begin{bmatrix}1,1 … 1\\end{bmatrix}$ 令式（3）等于0， 可以得到 $w^{\\star} = (X^TX)^{-1}X^T\\hat{Y}$, 此时 $A = X^TX$ 必须是非奇异矩阵。但是实际情况下，可能存在A并不是满秩的，比如特征的维度大于样本数。 因此，此时可能会接触多个w，能够使均方误差最小化。选择哪一个作为最优的w，由算法的归纳偏好来决定，比如引入正则化项。 2. 对数似然估计使用线性回归做分类问题，以二分类问题为例。 可以将其结果y映射到[0,1]区间，然后规定其值大于0.5作为正例，否则作为反例。 如何选择映射函数 f 呢，这里可以采用 sigmoid 函数：$$\\begin{aligned} f &amp;= \\sigma(z) = \\frac{1}{1 + \\exp{-z}} \\ z &amp;= w^Tx + b \\\\end{aligned}$$其曲线为：sigmoid 曲线此时，将f看作是概率分布，可以采用最大似然概率作为性能度量（它等价于最小化交叉熵）等价于优化问题： $$\\min_{w,b} L(w,b) = -\\sum_i^n(\\hat{y}_i\\log(f(x_i)) + (1-\\hat{y}_i)\\log(1 - f(x_i)))$$ 分别对 w,b求偏导数可得： $$ \\begin{aligned} \\frac{dL}{dw} &= -\\sum_i^n(\\frac{\\hat{y}_i}{f}\\frac{df}{dw} - \\frac{1-\\hat{y}_i}{1-f}\\frac{df}{dw}) \\\\ &= -\\sum_i^n(\\frac{\\hat{y}_i}{f}f(1-f)x_i - \\frac{1-\\hat{y}_i}{1-f}f(1-f)x_i )) \\\\ &= -\\sum_i^n(\\hat{y}_i - f)x_i \\qquad (5) \\\\ \\frac{dL}{db} &= -\\sum_i^n(\\hat{y}_i - f) \\qquad (6) \\end{aligned} $$ 分别对为w,b求偏导数其中$f = f(x_i)$。 可以发现与线性回归的偏导数有一样的格式。 3. LDA 线性判别分析LDA（Linear Discriminant Analysis), 又称为 fisher 判别分析 。它的主要思想是，将给定的样本，投影到一条直线上，并且保证类内的投影点越接近越好，但是类间的投影点越分散越好。类内投影点，越接近越好，可以采用方差（协方差举证）指标来衡量，即使其同类样本之间的协反差尽可能的小。对于类间的投影点，越分散越好。这里可以采用不同类别之间的均值相差尽可能的大。总结起来的公式为: $$\\max_{w} J(w) = \\frac{||w^T\\mu_0 - w^T\\mu_1||}{w^T\\Sigma_0w + w^T\\Sigma_1w}$$ LDA的度量函数w 代表被投影的直线。 令$S_w = \\Sigma_0 + \\Sigma_1$,$S_b = (\\mu_0 - \\mu_1)(\\mu_0 - \\mu_1)^T$则$J(w) = \\frac{w^TS_bw}{w^TS_ww}$。由于w代表被投影的直线，只与其方向有关，不妨设置$w^TS_ww = 1$, 则转化为以下有约束的优化问题 $$\\begin{aligned}&amp;\\min_{w}\\quad -w^TS_bw \\&amp;s.t \\quad w^TS_ww = 1\\end{aligned}$$ 可以使用拉格朗日乘法公式，可得: $$\\begin{aligned}L(w) &amp;= -w^TS_bw + \\lambda(w^TS_ww - 1) \\\\frac{dL}{dw} &amp;= -2S_bw + 2\\lambda S_ww = 0 \\&amp; S_w^{-1}S_bw = \\lambda w\\end{aligned}$$ 易知，$\\lambda$ 为矩阵 $S_w^{-1}S_w$的特征值和特征向量。 求解$S_w^{-1}$时，要考虑其稳定性，通常采用其SVD分解。即$S_w^{-1} = V\\Sigma^{-1}U^T$。","link":"/2020/12/26/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"},{"title":"拉格朗日乘法和KKT条件","text":"千秋摄于黄山 最近看了 《最优化理论与方法》, 学习了常见的优化问题以及解决方法。同时，之前学习 SVM 时，对拉格朗日乘法以及 KKT 条件有一些疑问，这里做一下学习笔记，便于后面复习和理解！ Notion: 同时参考了网上找到的一个PPT（见附件），觉得写的相当棒！ 主要设计四种优化问题： 无约束优化 等式约束优化 不等式约束优化 等式和不等式约束优化 讨论的对象是优化问题的标准形式: $$ \\begin{aligned} &\\min_{x\\in X } \\quad f(x) \\\\ &s.t \\quad \\begin{cases} g_i(x) = 0 \\quad i = 1,2...\\\\ h_j(x)","link":"/2021/08/18/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E4%B9%98%E6%B3%95%E5%92%8CKKT%E6%9D%A1%E4%BB%B6/"},{"title":"Soft-SVM","text":"由于随机噪声或者误差等问题，因此允许一些点不符合不等式约束条件。理想条件下(hard-svm), 优化问题如下: $$\\begin{aligned}&amp;\\min_{w,b} \\frac{w^Tw}{2} \\s.t.\\quad &amp; y_i(w^Tx_i + b) - 1 &gt;= 0 \\quad i = 1,2,…,n \\\\end{aligned}\\tag{1}$$ 我们允许其存在一定的违规量，这里引入松弛变量 $\\eta &gt;= 0$, 优化问题变为: $$\\begin{aligned}&amp; \\min_{w,b} \\frac{w^Tw}{2} + C\\sum_i^n \\eta_i \\s.t. \\quad &amp; y_i(w^Tx_i + b) - 1 + \\eta_i &gt;= 0\\&amp; \\eta_i &gt;= 0\\ \\tag{2} \\end{aligned}$$ 其中 C &gt; 0 ,成为惩罚参数，当C比较大时对误分类的惩罚较大，C较小时对误分类的惩罚较小，其值通常由应用决定。优化问题 (2) 其实也是个凸二次规划问题，其求解与Hard-SVM一样 1. Lagrange Multiplier对应的拉格朗日乘法公式为 $$\\begin{aligned}L(w,b,\\eta,\\delta) &amp;= \\frac{w^Tw}{2} + C\\sum_i^n \\eta_i - \\sum_i^n \\alpha_i( y_i(w^Tx_i + b) - 1 + \\eta_i) - \\sum_i^n \\delta_i\\eta_i \\\\alpha_i &amp;&gt;= 0 \\quad \\delta_i &gt;=0 \\\\end{aligned} \\tag{3}$$ $\\theta_p(w,b) = \\max_{\\eta,\\delta}L(w,b,\\eta,\\delta)$ 则优化问题（2）可以写作： $$\\begin{aligned}&amp; \\min_{w,b} \\theta_p(w,b) = \\min_{w,b} \\max_{\\eta,\\delta} L(w,b,\\eta,\\delta) \\s.t.\\quad &amp; \\alpha_i &gt;= 0 \\quad \\delta_i &gt;=0 \\ &amp; y_i(w^Tx_i + b) - 1 + \\eta_i &gt;= 0\\&amp; \\eta_i &gt;= 0\\\\end{aligned} \\tag{4}$$ 其对偶问题为： $$\\begin{aligned}&amp;\\max_{\\eta,\\delta} \\min_{w,b} L(w,b,\\eta,\\delta)\\s.t. \\quad &amp; \\eta_i &gt;=0 \\quad \\delta_i &gt;=0 \\ &amp; y_i(w^Tx_i + b) - 1 + \\eta_i &gt;= 0\\&amp; \\eta_i &gt;= 0\\\\end{aligned}\\tag{5}$$ 证明见Hard-SVM求解过程 求解 $\\min_{w,b} L(w,b,\\eta,\\del)$, 分别对w和b求导，并令其偏导数为0 $$\\begin{aligned}\\frac{dL}{dw} &amp;= w - \\sum_i^n\\alpha_iy_ix_i = 0\\\\frac{dL}{db} &amp;= -\\sum_i^n\\alpha_iy_i = 0\\\\end{aligned}\\tag{6}$$ 将(6)带入到 L 中，得到 G","link":"/2020/10/10/SVM(2)_%20Soft-SVM/"},{"title":"Hard-SVM","text":"支持向量机（Support Vector Machine）， 是一种解决分类和回归的经典机器学习模型。以解决分类问题为例，它的核心思想是，最大化输入向量到超平面的间隔。 之前在 感知机 里面我们已经了解到其思想为找到一个超平面，来划分特征空间为正负空间，从而实现分类的目的（如下图）。但是，这样的超平面不只一个，怎么来从中找到一个最优的超平面呢？ 如何评价超平面的优劣呢？这就是SVM解决分类问题的思想。 1. Hard-SVM1.1 模型SVM的思想，通俗来讲是，最大化 Margin(x), Margin(x)代表为，所有点到超平面距离中的最小距， 即 $\\min{Distance(x_i)}$ i = 1,2,3…,n化为最优化的标准型为： $$\\max_{w,b}Margin(x) = \\max_{w,b} \\min_{x_i}Distance(x_i) \\= \\max_{w,b}\\min_{x_i} \\frac{|w^Tx_i + b|}{||w||} \\s.t. \\quad y_i(w^Tx_i + b) &gt; 0$$ 由于 y = 1 or -1， 且分类正确时 $y（w^x + b）&gt; 0$, 因此 $｜w^T + b｜ = y(w^T + b)$这里假设，$y(w^Tx +b ) &gt;= \\sigma &gt; 0$, 此时问题改写为：$\\max_{w,b} \\frac{\\sigma}{||w||} \\s.t. \\quad y_i(w^Tx_i + b) &gt;= \\sigma$ 由于我们可以等比例的修改 w和b是的，$\\sigma$ 变为 1， 这样做并不改变问题的解。同时，$\\frac{1}{||w||}$等同于$\\frac{w^Tw}{2}$，此时问题修改为： $$ \\begin{aligned} &\\min_{w,b}\\frac{w^Tw}{2} \\\\ s.t. \\quad &y_i(w^Tx_i + b) -1 >= 0 \\end{aligned} \\tag{2} $$ 由于该问题是典型的二次优化问题，可以采用优化工具包来解决，也可以转化为对偶问题解决。 2. 策略如何求解式（1）中的二次优化问题，这里可以采用拉格朗日乘法来解决。采用拉格朗日乘法有一个前提，即该问题满足 KKT 条件 2.1 Dual Problem优化问题的标准形式为: $\\max_{x} f(x) \\s.t. \\quad h(x_i) = 0 \\\\quad g(x_i) &gt;= 0 \\quad(2)\\$ 这里引入广义拉格朗日乘法: $$L(x,\\lambda,\\eta) = f(x) - \\sum_i \\lambda_ih(x_i) - \\sum_i\\eta_ig(x_i) \\quad(3) \\\\eta_i &gt;= 0$$ 考虑x的函数$\\theta_p(x) = \\max_{\\lambda,\\eta,\\eta &gt;= 0} L(x,\\lambda,\\eta)$如果x不满足式（2）中的约束条件，即存在 $h(x) \\neq 0$ 或者 $g(x) &lt; 0$, 此时总存在 一个$\\lambda$ 或者$\\eta$ 使得， $\\eta g(x) \\rightarrow +\\infty \\\\lambda h(x) \\rightarrow +\\infty$而当 x符合（2）中条件时，$\\theta_P(x) = f(x)$。因此考虑极小化问题， $$\\begin{aligned}&amp;\\min_{x}{f(x)} = \\min_{x}{\\theta_p(x)} = \\min_{x} \\max_{\\lambda,\\eta,\\eta &gt;= 0}{L(x,\\eta,\\lambda)} \\&amp;\\text{Dual Problem} \\max_{\\lambda, \\eta; \\eta &gt;= 0} \\min_{x L(x,\\eta,\\lambda)}\\end{aligned}$$ 设：$p^{\\star} = \\min_{x}\\theta_p(x) \\d^{\\star} = \\max_{\\lambda, \\eta;\\eta &gt;= 0} \\min_{x} L(x,\\eta,\\lambda)$则有： $$\\min_{x}L(x,\\eta,\\lambda) &lt;= L(x,\\eta,\\lambda) &lt;= \\max_{\\lambda,\\eta,\\eta &gt;= 0}{L(x,\\eta,\\lambda)} \\\\rightarrow \\quad d^{\\star} &lt;= p^{\\star}$$ 当 优化问题的解满足 KKT条件时， $d^{\\star} = p^{\\star}$ 2.2 KKT条件把下面的条件记作 KKT 条件： $$ \\begin{aligned} \\triangledown L(x,\\lambda,\\eta) = 0 \\\\ \\eta_i >= 0 \\\\ \\eta_ig(x_i) = 0 \\\\ g(x_i) >=0 \\\\ h(x_i) = 0 \\end{aligned} $$ 证明： $当g(x_i) &gt; 0 时，此时条件 g(x_i) 不起作用，为等式约束，此时 \\eta_i = 0$ $当 g(x_i) = 0时，此时 \\eta_ig(x_i) = 0$ 2.3 模型求解因此，原问题可以转化为对偶问题 $$ \\begin{aligned} L(w,b,\\lambda) = \\frac{w^Tw}{2} - \\sum_i^n \\lambda_i(y_i(w^Tx_i + b) - 1) \\quad(3)\\\\ \\min_{w,b}\\max_{\\lambda}L(w,b,\\lambda) \\rightarrow \\max_{\\lambda}\\min_{w,b}L(w,b,\\lambda) \\end{aligned} $$ 求解： 对 w，b 求偏导数并令其等于0 $$ \\begin{aligned} \\frac{dL}{dw} = w - \\sum_i^n\\lambda_iy_ix_i = 0 \\quad(4)\\\\ \\frac{dL}{db} = -\\sum_i^n \\lambda_iy_i = 0 \\quad(5) \\end{aligned} $$ 将（4),(5）带入到(3)中，可得： $$ \\begin{aligned} G(\\lambda) &= \\frac{\\sum_i^n\\lambda_iy_ix_i^T\\sum_i^n\\lambda_iy_ix_i}{2} - \\sum_i^n \\lambda_i(y_i(\\sum_j^n\\lambda_jy_jx_j^Tx_i + b) - 1) \\\\ &= \\frac{\\sum_i^n\\lambda_iy_ix_i^T\\sum_i^n\\lambda_iy_ix_i}{2} - \\sum_i^n \\lambda_i(y_i(\\sum_j^n\\lambda_jy_jx_j^Tx_i)) +\\sum_i^n\\lambda_i \\\\ &= \\sum_i^n\\lambda_i - \\frac{\\sum_i^n\\sum_j^n\\lambda_jy_ix_jx_i^T\\lambda_iy_i}{2} \\\\ \\end{aligned}\\tag{6} $$ 此时优化问题为: $$\\begin{aligned} &\\max_{\\lambda} G(\\lambda) \\\\ s.t.\\quad & \\sum_i^n \\lambda_iy_i = 0 \\\\ &\\lambda_i >= 0 \\end{aligned} \\tag{7} $$ 对G函数求极大值，即可得到解 $\\lambda^{\\star}$， 带入到 (4),(5) 可以求解出 $w^{\\star} = \\sum_i^n \\lambda^{\\star}_iy_ix_i$。 求解b, 易知存在 $\\lambda_j \\neq 0$（可以使用反证法证明，若全部$\\lambda$均为0，则 w为0，而w=0不是原始优化问题的解）。此时，有$y_i(w^{\\star} \\cdot x_i + b ) - 1 = 0$,可得 $b^{\\star} = y_i^2 - w^{\\star} \\cdot x_i$ 。 2.4 支撑向量由2.3 第3、4步可知，如果只保留 $\\lambda \\neq 0, \\quad y(w^Tx + b) - 1 = 0$ 对应的x,y，求得的结果w和b不变。 因此把这些向量称之为支撑向量，即 support vector。 reference 统计学习方法. 李航. chapter 7 机器学习. 周志华","link":"/2020/09/27/SVM(1)_%20Hard-SVM/"},{"title":"Disjoint Set (Union-Find )","text":"并查集数据结构 只有 find 和 union 操作, 它用于处理一些不交集的 合并 及 查询 问题。 find : 确定元素所在的集合 union: 合并两个不相交集合 并查集的数据结构 初始化 初始化时，每个节点都是一个集合。 union(x,y) 合并的过程是： 先找到x（y）节点的祖先节点(find(x)) [xp，yp] ，然后比较祖先节点是否相同，若相同则完成合并。负责将其中的祖先节点 (xp) 的父节点指向另一个祖先节点 (yp)：parent[xp] = yp; 一个简单的并查集模版 12345678910111213141516171819202122232425262728int parent[MAXLEN];// 初始化通常将其自身作为父节点void makeSet(int k){ for(int i =0;i&lt;k;i++) parent[i] = i;}// 查找其根节点int find(int x){ if(parent[x] != x){ parent[x] = find(parent[x]); } return parent[x];}// 合并两个不相交集合void union(int x,int y){ int xp = find(x); int yp = find(y); if(xp != yp){ parent[xp] = yp; } while(parent[x]!=yp){ int t = parent[x]; parent[x]=yp; x = t; }} 路径压缩 这样的确可以达成目的，但是显然效率实在太低。为什么呢？因为我们使用了太多没用的信息，我的祖先是谁与我父亲是谁没什么关系，这样一层一层找太浪费时间，不如我直接当祖先的儿子，问一次就可以出结果了。甚至祖先是谁都无所谓，只要这个人可以代表我们家族就能得到想要的效果。把在路径上的每个节点都直接连接到根上，这就是路径压缩。 并查集的应用1. 统计不相交集合的个数 200. 岛屿数量 给你一个由 ‘1’（陆地）和 ‘0’（水）组成的的二维网格，请你计算网格中岛屿的数量。岛屿总是被水包围，并且每座岛屿只能由水平方向和/或竖直方向上相邻的陆地连接形成。此外，你可以假设该网格的四条边均被水包围。 示例 1： 输入：grid = [ [&quot;1&quot;,&quot;1&quot;,&quot;1&quot;,&quot;1&quot;,&quot;0&quot;], [&quot;1&quot;,&quot;1&quot;,&quot;0&quot;,&quot;1&quot;,&quot;0&quot;], [&quot;1&quot;,&quot;1&quot;,&quot;0&quot;,&quot;0&quot;,&quot;0&quot;], [&quot;0&quot;,&quot;0&quot;,&quot;0&quot;,&quot;0&quot;,&quot;0&quot;] ] 输出：1 示例 2： 输入：grid = [ [&quot;1&quot;,&quot;1&quot;,&quot;0&quot;,&quot;0&quot;,&quot;0&quot;], [&quot;1&quot;,&quot;1&quot;,&quot;0&quot;,&quot;0&quot;,&quot;0&quot;], [&quot;0&quot;,&quot;0&quot;,&quot;1&quot;,&quot;0&quot;,&quot;0&quot;], [&quot;0&quot;,&quot;0&quot;,&quot;0&quot;,&quot;1&quot;,&quot;1&quot;] ] 输出：3 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879#include&lt;iostream&gt;#include&lt;vector&gt;#include&lt;string&gt;using namespace std;class Solution {public: struct Node{ int x; int y; Node():x(0),y(0){}; Node(int x, int y):x(x),y(y){}; bool equal(Node node){return node.x == x &amp;&amp; node.y == y;}; }; Node find(vector&lt;vector&lt;Node&gt;&gt; &amp;parent, Node node){ while(!node.equal(parent[node.x][node.y])){ node = parent[node.x][node.y]; } return node; } bool isParent(vector&lt;vector&lt;Node&gt;&gt; &amp;parent, Node node){ return node.equal(parent[node.x][node.y]); } void unionNode(vector&lt;vector&lt;Node&gt;&gt; &amp;parent, Node x, Node y){ Node xp = find(parent,x); Node yp = find(parent,y); if(!xp.equal(yp)) { parent[xp.x][xp.y] = yp; } while(!x.equal(yp)){ Node node = parent[x.x][x.y]; parent[x.x][x.y] = yp; x = node; } } int numIslands(vector&lt;vector&lt;char&gt;&gt;&amp; grid) { int m = grid.size(), n = grid[0].size(); vector&lt;vector&lt;Node&gt;&gt; parent(m,vector&lt;Node&gt;(n,Node())); for(int i =0;i&lt;m;i++){ for(int j=0;j&lt;n;j++){ parent[i][j] = Node(i,j); } } vector&lt;int&gt; xd = {-1,0,1,0}; vector&lt;int&gt; yd = {0,1,0,-1}; for(int i =0;i&lt;m;i++){ for(int j=0;j&lt;n;j++){ if(grid[i][j] == '0') continue; for(int k =0;k&lt; 4;k++){ int x = xd[k] + i; int y = yd[k] + j; if(x &gt;=0 &amp;&amp; x &lt; m &amp;&amp; y &gt;=0 &amp;&amp; y &lt; n){ if(grid[x][y] == '1') unionNode(parent,Node(i,j),Node(x,y)); } } } } int ans = 0; for(int i =0;i&lt;m;i++){ for(int j = 0;j&lt;n;j++){ if(grid[i][j] == '1' &amp;&amp; isParent(parent,Node(i,j))) ans++; } } return ans; }}; Referencehttps://oi-wiki.org/ds/dsu/ https://leetcode-cn.com/problems/number-of-provinces/solution/jie-zhe-ge-wen-ti-ke-pu-yi-xia-bing-cha-0unne/","link":"/2022/01/12/Disjoint%20S%208c71a/"}],"tags":[{"name":"leetcode","slug":"leetcode","link":"/tags/leetcode/"},{"name":"algorithms","slug":"algorithms","link":"/tags/algorithms/"},{"name":"程序员面试宝典","slug":"程序员面试宝典","link":"/tags/%E7%A8%8B%E5%BA%8F%E5%91%98%E9%9D%A2%E8%AF%95%E5%AE%9D%E5%85%B8/"},{"name":"sort","slug":"sort","link":"/tags/sort/"},{"name":"usage","slug":"usage","link":"/tags/usage/"},{"name":"hpc","slug":"hpc","link":"/tags/hpc/"},{"name":"parallel computers","slug":"parallel-computers","link":"/tags/parallel-computers/"},{"name":"introduction","slug":"introduction","link":"/tags/introduction/"},{"name":"machine learning","slug":"machine-learning","link":"/tags/machine-learning/"},{"name":"classification","slug":"classification","link":"/tags/classification/"},{"name":"probability","slug":"probability","link":"/tags/probability/"},{"name":"deep learning","slug":"deep-learning","link":"/tags/deep-learning/"},{"name":"perceptron","slug":"perceptron","link":"/tags/perceptron/"},{"name":"regression","slug":"regression","link":"/tags/regression/"},{"name":"linear regression","slug":"linear-regression","link":"/tags/linear-regression/"},{"name":"最优化理论","slug":"最优化理论","link":"/tags/%E6%9C%80%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA/"},{"name":"SVM","slug":"SVM","link":"/tags/SVM/"},{"name":"并查集","slug":"并查集","link":"/tags/%E5%B9%B6%E6%9F%A5%E9%9B%86/"}],"categories":[{"name":"Daily Check","slug":"Daily-Check","link":"/categories/Daily-Check/"},{"name":"说明书","slug":"说明书","link":"/categories/%E8%AF%B4%E6%98%8E%E4%B9%A6/"},{"name":"parallel computers","slug":"parallel-computers","link":"/categories/parallel-computers/"},{"name":"machine learning","slug":"machine-learning","link":"/categories/machine-learning/"},{"name":"deep learning","slug":"machine-learning/deep-learning","link":"/categories/machine-learning/deep-learning/"},{"name":"数据结构","slug":"数据结构","link":"/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]}